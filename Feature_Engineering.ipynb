{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "03. Feature Engineering.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "VYi7wjk52BvM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to create features from the raw text so we can train the machine learning models. The steps followed are:\n",
        "\n",
        "1. **Text Cleaning and Preparation**: cleaning of special characters, downcasing, punctuation signs. possessive pronouns and stop words removal and lemmatization. \n",
        "2. **Label coding**: creation of a dictionary to map each category to a code.\n",
        "3. **Train-test split**: to test the models on unseen data.\n",
        "4. **Text representation**: use of TF-IDF scores to represent text."
      ],
      "metadata": {
        "id": "lHa2dOsE2BvO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import chi2\n",
        "import numpy as np"
      ],
      "outputs": [],
      "metadata": {
        "id": "pUNHkXFW2BvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all we'll load the dataset:"
      ],
      "metadata": {
        "id": "a8MJHily2BvQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHS6HGpvHWAw",
        "outputId": "b481969e-3e51-4016-94c5-567f2b5398eb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pathcolab='/content/drive/MyDrive/AI Projects/Feature Engineering/ICE 1/Already_Created_Set/Latest-News-Classifier/0. Latest News Classifier/02. Exploratory Data Analysis/'\n",
        "path_df = 'News_dataset.pickle'\n",
        "with open(pathcolab+path_df, 'rb') as data:\n",
        "    df = pickle.load(data)"
      ],
      "outputs": [],
      "metadata": {
        "id": "y0d2LjtL2BvQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Name</th>\n",
              "      <th>Content</th>\n",
              "      <th>Category</th>\n",
              "      <th>Complete_Filename</th>\n",
              "      <th>id</th>\n",
              "      <th>News_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>001.txt</td>\n",
              "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
              "      <td>business</td>\n",
              "      <td>001.txt-business</td>\n",
              "      <td>1</td>\n",
              "      <td>2559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>002.txt</td>\n",
              "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
              "      <td>business</td>\n",
              "      <td>002.txt-business</td>\n",
              "      <td>1</td>\n",
              "      <td>2251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>003.txt</td>\n",
              "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
              "      <td>business</td>\n",
              "      <td>003.txt-business</td>\n",
              "      <td>1</td>\n",
              "      <td>1551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>004.txt</td>\n",
              "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
              "      <td>business</td>\n",
              "      <td>004.txt-business</td>\n",
              "      <td>1</td>\n",
              "      <td>2411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>005.txt</td>\n",
              "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
              "      <td>business</td>\n",
              "      <td>005.txt-business</td>\n",
              "      <td>1</td>\n",
              "      <td>1569</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  File_Name                                            Content  ... id News_length\n",
              "0   001.txt  Ad sales boost Time Warner profit\\n\\nQuarterly...  ...  1        2559\n",
              "1   002.txt  Dollar gains on Greenspan speech\\n\\nThe dollar...  ...  1        2251\n",
              "2   003.txt  Yukos unit buyer faces loan claim\\n\\nThe owner...  ...  1        1551\n",
              "3   004.txt  High fuel prices hit BA's profits\\n\\nBritish A...  ...  1        2411\n",
              "4   005.txt  Pernod takeover talk lifts Domecq\\n\\nShares in...  ...  1        1569\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "9rvRaGDQ2BvR",
        "outputId": "2f173f8e-c043-46a4-b18e-116fdea83eca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And visualize one sample news content:"
      ],
      "metadata": {
        "id": "m12xozYU2BvS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.loc[1]['Content']"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Dollar gains on Greenspan speech\\n\\nThe dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\\n\\nAnd Alan Greenspan highlighted the US government\\'s willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan\\'s speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. \"I think the chairman\\'s taking a much more sanguine view on the current account deficit than he\\'s taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York. \"He\\'s taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"\\n\\nWorries about the deficit concerns about China do, however, remain. China\\'s currency remains pegged to the dollar and the US currency\\'s sharp falls in recent months have therefore made Chinese export prices highly competitive. But calls for a shift in Beijing\\'s policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg. The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy. In the meantime, the US Federal Reserve\\'s decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates. The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar. The recent falls have partly been the result of big budget deficits, as well as the US\\'s yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.'"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "--5bdZ8D2BvT",
        "outputId": "3a4a3c35-c2d0-4d16-c6af-82a7a022341b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Text cleaning and preparation"
      ],
      "metadata": {
        "id": "L4yhCnVQ2BvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Special character cleaning\n",
        "\n",
        "We can see the following special characters:\n",
        "\n",
        "* ``\\r``\n",
        "* ``\\n``\n",
        "* ``\\`` before possessive pronouns (`government's = government\\'s`)\n",
        "* ``\\`` before possessive pronouns 2 (`Yukos'` = `Yukos\\'`)\n",
        "* ``\"`` when quoting text"
      ],
      "metadata": {
        "id": "8GOusJir2BvU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# \\r and \\n\n",
        "df['Content_Parsed_1'] = df['Content'].str.replace(\"\\r\", \" \")\n",
        "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
        "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"    \", \" \")"
      ],
      "outputs": [],
      "metadata": {
        "id": "wCX0_D5z2BvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regarding 3rd and 4th bullet, although it seems there is a special character, it won't affect us since it is not a *real* character:"
      ],
      "metadata": {
        "id": "Q_xdwLZy2BvV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "text = \"Mr Greenspan\\'s\"\n",
        "text"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Mr Greenspan's\""
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G-p9T8LI2BvV",
        "outputId": "060a8e40-85fe-49d5-f952-2df34589dfd9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# \" when quoting text\n",
        "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace('\"', '')"
      ],
      "outputs": [],
      "metadata": {
        "id": "2OIqQHId2BvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Upcase/downcase"
      ],
      "metadata": {
        "id": "9otNqtmA2BvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll downcase the texts because we want, for example, `Football` and `football` to be the same word."
      ],
      "metadata": {
        "id": "24umiWiK2BvW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Lowercasing the text\n",
        "df['Content_Parsed_2'] = df['Content_Parsed_1'].str.lower()"
      ],
      "outputs": [],
      "metadata": {
        "id": "8CnRUOHV2BvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. Punctuation signs"
      ],
      "metadata": {
        "id": "kRObHaxo2BvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punctuation signs won't have any predicting power, so we'll just get rid of them."
      ],
      "metadata": {
        "id": "7tym8syW2BvY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "punctuation_signs = list(\"?:!.,;\")\n",
        "df['Content_Parsed_3'] = df['Content_Parsed_2']\n",
        "\n",
        "for punct_sign in punctuation_signs:\n",
        "    df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(punct_sign, '')"
      ],
      "outputs": [],
      "metadata": {
        "id": "2PnlxszL2BvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By doing this we are messing up with some numbers, but it's no problem since we aren't expecting any predicting power from them."
      ],
      "metadata": {
        "id": "SJGe2BWu2BvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4. Possessive pronouns"
      ],
      "metadata": {
        "id": "3re7YVqf2BvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also remove possessive pronoun terminations:"
      ],
      "metadata": {
        "id": "PCV5C6ax2BvZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df['Content_Parsed_4'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "2b3tKC_V2BvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5. Stemming and Lemmatization\n",
        "\n",
        "Since stemming can produce output words that don't exist, we'll only use a lemmatization process at this moment. Lemmatization takes into consideration the morphological analysis of the words and returns words that do exist, so it will be more useful for us."
      ],
      "metadata": {
        "id": "K1INiQ0O2BvZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Downloading punkt and wordnet from NLTK\n",
        "nltk.download('punkt')\n",
        "print(\"------------------------------------------------------------\")\n",
        "nltk.download('wordnet')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "------------------------------------------------------------\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdc9e5TW2Bva",
        "outputId": "7b03d4e5-bf44-4c05-dc90-0f01460121db"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Saving the lemmatizer into an object\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "outputs": [],
      "metadata": {
        "id": "tm2mH_qT2Bva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to lemmatize, we have to iterate through every word:"
      ],
      "metadata": {
        "id": "ze20_SBi2Bva"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "nrows = len(df)\n",
        "lemmatized_text_list = []\n",
        "\n",
        "for row in range(0, nrows):\n",
        "    \n",
        "    # Create an empty list containing lemmatized words\n",
        "    lemmatized_list = []\n",
        "    \n",
        "    # Save the text and its words into an object\n",
        "    text = df.loc[row]['Content_Parsed_4']\n",
        "    text_words = text.split(\" \")\n",
        "\n",
        "    # Iterate through every word to lemmatize\n",
        "    for word in text_words:\n",
        "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
        "        \n",
        "    # Join the list\n",
        "    lemmatized_text = \" \".join(lemmatized_list)\n",
        "    \n",
        "    # Append to the list containing the texts\n",
        "    lemmatized_text_list.append(lemmatized_text)"
      ],
      "outputs": [],
      "metadata": {
        "id": "hYCv9A1M2Bvb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df['Content_Parsed_5'] = lemmatized_text_list"
      ],
      "outputs": [],
      "metadata": {
        "id": "uzWvflYr2Bvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although lemmatization doesn't work perfectly in all cases (as can be seen in the example below), it can be useful."
      ],
      "metadata": {
        "id": "_WWg9N2w2Bvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6. Stop words"
      ],
      "metadata": {
        "id": "jhBooCV12Bvd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Downloading the stop words list\n",
        "nltk.download('stopwords')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mhMsJlF2Bvd",
        "outputId": "81f1b372-7b9d-48bd-eb4c-28791ec44110"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Loading the stop words in english\n",
        "stop_words = list(stopwords.words('english'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "fzKs339z2Bvf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "stop_words[0:10]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4urGuXA2Bvh",
        "outputId": "62e1a909-1fd5-4c71-f087-af7ee307625f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To remove the stop words, we'll handle a regular expression only detecting whole words, as seen in the following example:"
      ],
      "metadata": {
        "id": "kC0-8vrK2Bvi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "example = \"me eating a meal\"\n",
        "word = \"me\"\n",
        "\n",
        "# The regular expression is:\n",
        "regex = r\"\\b\" + word + r\"\\b\"  # we need to build it like that to work properly\n",
        "\n",
        "re.sub(regex, \"StopWord\", example)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'StopWord eating a meal'"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "49SpvMA12Bvj",
        "outputId": "01df2588-7985-4380-b6da-0dc861ad9faa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now loop through all the stop words:"
      ],
      "metadata": {
        "id": "UbNZM72R2Bvk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df['Content_Parsed_6'] = df['Content_Parsed_5']\n",
        "\n",
        "for stop_word in stop_words:\n",
        "\n",
        "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
        "    df['Content_Parsed_6'] = df['Content_Parsed_6'].str.replace(regex_stopword, '')"
      ],
      "outputs": [],
      "metadata": {
        "id": "FIAk8aeF2Bvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have some dobule/triple spaces between words because of the replacements. However, it's not a problem because we'll tokenize by the spaces later."
      ],
      "metadata": {
        "id": "rN3QrPnW2Bvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example, we'll show an original news article and its modifications throughout the process:"
      ],
      "metadata": {
        "id": "EkAf6QI72Bvl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.loc[5]['Content']"
      ],
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'An autonomous Uber car killed a woman in the street in Arizona, police said, in what appears to be the first reported fatal crash involving a self-driving vehicle and a pedestrian in the US.\\nThe company said it was pausing its self-driving car operations in Phoenix, Pittsburgh, San Francisco and Toronto.\\nPhotograph: APThe self-driving technology is supposed to detect pedestrians, cyclists and others and prevent crashes.\\nSimpson said he was unaware of any previous fatal crashes involving an autonomous vehicle and a pedestrian.\\nEarlier this year, California regulators approved the testing of self-driving cars on public roads without human drivers monitoring inside.'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Special character cleaning"
      ],
      "metadata": {
        "id": "CHYK2kTA2Bvl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.loc[5]['Content_Parsed_1']"
      ],
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'An autonomous Uber car killed a woman in the street in Arizona, police said, in what appears to be the first reported fatal crash involving a self-driving vehicle and a pedestrian in the US. The company said it was pausing its self-driving car operations in Phoenix, Pittsburgh, San Francisco and Toronto. Photograph: APThe self-driving technology is supposed to detect pedestrians, cyclists and others and prevent crashes. Simpson said he was unaware of any previous fatal crashes involving an autonomous vehicle and a pedestrian. Earlier this year, California regulators approved the testing of self-driving cars on public roads without human drivers monitoring inside.'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Upcase/downcase"
      ],
      "metadata": {
        "id": "e91KFxxj2Bvn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.loc[5]['Content_Parsed_2']"
      ],
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'an autonomous uber car killed a woman in the street in arizona, police said, in what appears to be the first reported fatal crash involving a self-driving vehicle and a pedestrian in the us. the company said it was pausing its self-driving car operations in phoenix, pittsburgh, san francisco and toronto. photograph: apthe self-driving technology is supposed to detect pedestrians, cyclists and others and prevent crashes. simpson said he was unaware of any previous fatal crashes involving an autonomous vehicle and a pedestrian. earlier this year, california regulators approved the testing of self-driving cars on public roads without human drivers monitoring inside.'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Punctuation signs"
      ],
      "metadata": {
        "id": "SF7b9WBC2Bvn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.loc[5]['Content_Parsed_3']"
      ],
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'an autonomous uber car killed a woman in the street in arizona police said in what appears to be the first reported fatal crash involving a self-driving vehicle and a pedestrian in the us the company said it was pausing its self-driving car operations in phoenix pittsburgh san francisco and toronto photograph apthe self-driving technology is supposed to detect pedestrians cyclists and others and prevent crashes simpson said he was unaware of any previous fatal crashes involving an autonomous vehicle and a pedestrian earlier this year california regulators approved the testing of self-driving cars on public roads without human drivers monitoring inside'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Possessive pronouns"
      ],
      "metadata": {
        "id": "O_-fej3Y2Bvo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.loc[5]['Content_Parsed_4']"
      ],
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'an autonomous uber car killed a woman in the street in arizona police said in what appears to be the first reported fatal crash involving a self-driving vehicle and a pedestrian in the us the company said it was pausing its self-driving car operations in phoenix pittsburgh san francisco and toronto photograph apthe self-driving technology is supposed to detect pedestrians cyclists and others and prevent crashes simpson said he was unaware of any previous fatal crashes involving an autonomous vehicle and a pedestrian earlier this year california regulators approved the testing of self-driving cars on public roads without human drivers monitoring inside'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Stemming and Lemmatization"
      ],
      "metadata": {
        "id": "PzBCo72T2Bvo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.loc[5]['Content_Parsed_5']"
      ],
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'an autonomous uber car kill a woman in the street in arizona police say in what appear to be the first report fatal crash involve a self-driving vehicle and a pedestrian in the us the company say it be pause its self-driving car operations in phoenix pittsburgh san francisco and toronto photograph apthe self-driving technology be suppose to detect pedestrians cyclists and others and prevent crash simpson say he be unaware of any previous fatal crash involve an autonomous vehicle and a pedestrian earlier this year california regulators approve the test of self-driving cars on public roads without human drivers monitor inside'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Stop words"
      ],
      "metadata": {
        "id": "sg5k4Xuc2Bvo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.loc[5]['Content_Parsed_6']"
      ],
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' autonomous uber car kill  woman   street  arizona police say   appear    first report fatal crash involve  self-driving vehicle   pedestrian   us  company say   pause  self-driving car operations  phoenix pittsburgh san francisco  toronto photograph apthe self-driving technology  suppose  detect pedestrians cyclists  others  prevent crash simpson say   unaware   previous fatal crash involve  autonomous vehicle   pedestrian earlier  year california regulators approve  test  self-driving cars  public roads without human drivers monitor inside'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can delete the intermediate columns:"
      ],
      "metadata": {
        "id": "MBy08hwY2Bvp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.head(1)"
      ],
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Name</th>\n",
              "      <th>Content</th>\n",
              "      <th>Category</th>\n",
              "      <th>id</th>\n",
              "      <th>News_length</th>\n",
              "      <th>Content_Parsed_1</th>\n",
              "      <th>Content_Parsed_2</th>\n",
              "      <th>Content_Parsed_3</th>\n",
              "      <th>Content_Parsed_4</th>\n",
              "      <th>Content_Parsed_5</th>\n",
              "      <th>Content_Parsed_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Tempe Police Department said it was invest...</td>\n",
              "      <td>We continue to assist investigators in any way...</td>\n",
              "      <td>autonomous car</td>\n",
              "      <td>1</td>\n",
              "      <td>799.0</td>\n",
              "      <td>We continue to assist investigators in any way...</td>\n",
              "      <td>we continue to assist investigators in any way...</td>\n",
              "      <td>we continue to assist investigators in any way...</td>\n",
              "      <td>we continue to assist investigators in any way...</td>\n",
              "      <td>we continue to assist investigators in any way...</td>\n",
              "      <td>continue  assist investigators   way  ”uber  ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           File_Name  \\\n",
              "0  The Tempe Police Department said it was invest...   \n",
              "\n",
              "                                             Content        Category  id  \\\n",
              "0  We continue to assist investigators in any way...  autonomous car   1   \n",
              "\n",
              "   News_length                                   Content_Parsed_1  \\\n",
              "0        799.0  We continue to assist investigators in any way...   \n",
              "\n",
              "                                    Content_Parsed_2  \\\n",
              "0  we continue to assist investigators in any way...   \n",
              "\n",
              "                                    Content_Parsed_3  \\\n",
              "0  we continue to assist investigators in any way...   \n",
              "\n",
              "                                    Content_Parsed_4  \\\n",
              "0  we continue to assist investigators in any way...   \n",
              "\n",
              "                                    Content_Parsed_5  \\\n",
              "0  we continue to assist investigators in any way...   \n",
              "\n",
              "                                    Content_Parsed_6  \n",
              "0   continue  assist investigators   way  ”uber  ...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "list_columns = [\"File_Name\", \"Category\", \"Complete_Filename\", \"Content\", \"Content_Parsed_6\"]\n",
        "df = df[list_columns]\n",
        "\n",
        "df = df.rename(columns={'Content_Parsed_6': 'Content_Parsed'})"
      ],
      "outputs": [],
      "metadata": {
        "id": "axvYjWZt2Bvp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.head()"
      ],
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File_Name</th>\n",
              "      <th>Category</th>\n",
              "      <th>Complete_Filename</th>\n",
              "      <th>Content</th>\n",
              "      <th>Content_Parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Tempe Police Department said it was invest...</td>\n",
              "      <td>autonomous car</td>\n",
              "      <td>The Tempe Police Department said it was invest...</td>\n",
              "      <td>We continue to assist investigators in any way...</td>\n",
              "      <td>continue  assist investigators   way  ”uber  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>On Sunday, the inevitable happened: An autonom...</td>\n",
              "      <td>autonomous car</td>\n",
              "      <td>On Sunday, the inevitable happened: An autonom...</td>\n",
              "      <td>Cars don’t see wellAutonomous cars don’t track...</td>\n",
              "      <td>cars ’ see wellautonomous cars ’ track  center...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Even before a driverless Uber vehicle struck a...</td>\n",
              "      <td>autonomous car</td>\n",
              "      <td>Even before a driverless Uber vehicle struck a...</td>\n",
              "      <td>The accident in Tempe, Arizona, was believed t...</td>\n",
              "      <td>accident  tempe arizona  believe    first tim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>On Sunday night, a woman died after she was hi...</td>\n",
              "      <td>autonomous car</td>\n",
              "      <td>On Sunday night, a woman died after she was hi...</td>\n",
              "      <td>On Sunday night, a woman died after she was hi...</td>\n",
              "      <td>sunday night  woman die    hit   self-driving...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A self-driving vehicle made by Uber has struck...</td>\n",
              "      <td>autonomous car</td>\n",
              "      <td>A self-driving vehicle made by Uber has struck...</td>\n",
              "      <td>Something unexpectedly entering the vehicle’s ...</td>\n",
              "      <td>something unexpectedly enter  vehicle’ path  p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           File_Name        Category  \\\n",
              "0  The Tempe Police Department said it was invest...  autonomous car   \n",
              "1  On Sunday, the inevitable happened: An autonom...  autonomous car   \n",
              "2  Even before a driverless Uber vehicle struck a...  autonomous car   \n",
              "3  On Sunday night, a woman died after she was hi...  autonomous car   \n",
              "4  A self-driving vehicle made by Uber has struck...  autonomous car   \n",
              "\n",
              "                                   Complete_Filename  \\\n",
              "0  The Tempe Police Department said it was invest...   \n",
              "1  On Sunday, the inevitable happened: An autonom...   \n",
              "2  Even before a driverless Uber vehicle struck a...   \n",
              "3  On Sunday night, a woman died after she was hi...   \n",
              "4  A self-driving vehicle made by Uber has struck...   \n",
              "\n",
              "                                             Content  \\\n",
              "0  We continue to assist investigators in any way...   \n",
              "1  Cars don’t see wellAutonomous cars don’t track...   \n",
              "2  The accident in Tempe, Arizona, was believed t...   \n",
              "3  On Sunday night, a woman died after she was hi...   \n",
              "4  Something unexpectedly entering the vehicle’s ...   \n",
              "\n",
              "                                      Content_Parsed  \n",
              "0   continue  assist investigators   way  ”uber  ...  \n",
              "1  cars ’ see wellautonomous cars ’ track  center...  \n",
              "2   accident  tempe arizona  believe    first tim...  \n",
              "3   sunday night  woman die    hit   self-driving...  \n",
              "4  something unexpectedly enter  vehicle’ path  p...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT:**\n",
        "\n",
        "We need to remember that our model will gather the latest news articles from different newspapers every time we want. For that reason, we not only need to take into account the peculiarities of the training set articles, but also possible ones that are present in the gathered news articles.\n",
        "\n",
        "For this reason, possible peculiarities have been studied in the *05. News Scraping* folder."
      ],
      "metadata": {
        "id": "5YqAE_Us2Bvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Label coding"
      ],
      "metadata": {
        "id": "rTLGqhaY2Bvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create a dictionary with the label codification:"
      ],
      "metadata": {
        "id": "PhZKy0Yu2Bvp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "category_codes = {\n",
        "    'business': 0,\n",
        "    'entertainment': 1,\n",
        "    'politics': 2,\n",
        "    'sport': 3,\n",
        "    'tech': 4\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "4EGhEbNn2Bvq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Category mapping\n",
        "df['Category_Code'] = df['Category']\n",
        "df = df.replace({'Category_Code':category_codes})"
      ],
      "outputs": [],
      "metadata": {
        "id": "hXRW0wd92Bvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train - test split"
      ],
      "metadata": {
        "id": "E50HmOEj2Bvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set apart a test set to prove the quality of our models. We'll do Cross Validation in the train set in order to tune the hyperparameters and then test performance on the unseen data of the test set."
      ],
      "metadata": {
        "id": "Rrtz19P82Bvq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['Content_Parsed'], \n",
        "                                                    df['Category_Code'], \n",
        "                                                    test_size=0.15, \n",
        "                                                    random_state=8)"
      ],
      "outputs": [],
      "metadata": {
        "id": "UfnoPlVt2Bvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we don't have much observations (only 2.225), we'll choose a test set size of 15% of the full dataset."
      ],
      "metadata": {
        "id": "EykHaOnB2Bvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Text representation"
      ],
      "metadata": {
        "id": "Q6Zu_TW92Bvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have various options:\n",
        "\n",
        "* Count Vectors as features\n",
        "* TF-IDF Vectors as features\n",
        "* Word Embeddings as features\n",
        "* Text / NLP based features\n",
        "* Topic Models as features"
      ],
      "metadata": {
        "id": "XKNoXqoh2Bvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use **TF-IDF Vectors** as features."
      ],
      "metadata": {
        "id": "YD-CVDuz2Bvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to define the different parameters:\n",
        "\n",
        "* `ngram_range`: We want to consider both unigrams and bigrams.\n",
        "* `max_df`: When building the vocabulary ignore terms that have a document\n",
        "    frequency strictly higher than the given threshold\n",
        "* `min_df`: When building the vocabulary ignore terms that have a document\n",
        "    frequency strictly lower than the given threshold.\n",
        "* `max_features`: If not None, build a vocabulary that only consider the top\n",
        "    max_features ordered by term frequency across the corpus.\n",
        "\n",
        "See `TfidfVectorizer?` for further detail."
      ],
      "metadata": {
        "id": "z-FWuy0V2Bvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It needs to be mentioned that we are implicitly scaling our data when representing it as TF-IDF features with the argument `norm`."
      ],
      "metadata": {
        "id": "RpfCyB8u2Bvr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Parameter election\n",
        "ngram_range = (1,2)\n",
        "min_df = 10\n",
        "max_df = 1.\n",
        "max_features = 300"
      ],
      "outputs": [],
      "metadata": {
        "id": "F7rJ3kvt2Bvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have chosen these values as a first approximation. Since the models that we develop later have a very good predictive power, we'll stick to these values. But it has to be mentioned that different combinations could be tried in order to improve even more the accuracy of the models."
      ],
      "metadata": {
        "id": "N6FFq8ri2Bvr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tfidf = TfidfVectorizer(encoding='utf-8',\n",
        "                        ngram_range=ngram_range,\n",
        "                        stop_words=None,\n",
        "                        lowercase=False,\n",
        "                        max_df=max_df,\n",
        "                        min_df=min_df,\n",
        "                        max_features=max_features,\n",
        "                        norm='l2',\n",
        "                        sublinear_tf=True)\n",
        "                        \n",
        "features_train = tfidf.fit_transform(X_train).toarray()\n",
        "labels_train = y_train\n",
        "print(features_train.shape)\n",
        "\n",
        "features_test = tfidf.transform(X_test).toarray()\n",
        "labels_test = y_test\n",
        "print(features_test.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1891, 300)\n",
            "(334, 300)\n"
          ]
        }
      ],
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1EJiq3t2Bvr",
        "outputId": "28e6ed1f-868a-43ab-d4dd-d6083062ddec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that we have fitted and then transformed the training set, but we have **only transformed** the **test set**."
      ],
      "metadata": {
        "id": "JsBLFyYo2Bvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the Chi squared test in order to see what unigrams and bigrams are most correlated with each category:"
      ],
      "metadata": {
        "id": "g4fcj9xj2Bvr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "import numpy as np\n",
        "\n",
        "for Product, category_id in sorted(category_codes.items()):\n",
        "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
        "    indices = np.argsort(features_chi2[0])\n",
        "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
        "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
        "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
        "    print(\"# '{}' category:\".format(Product))\n",
        "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
        "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
        "    print(\"\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 'autonomous car' category:\n",
            "  . Most correlated unigrams:\n",
            ". vehicles\n",
            ". self\n",
            ". cars\n",
            ". driving\n",
            ". autonomous\n",
            "  . Most correlated bigrams:\n",
            ". year old\n",
            ". self driving\n",
            "\n",
            "# 'business' category:\n",
            "  . Most correlated unigrams:\n",
            ". oil\n",
            ". market\n",
            ". bank\n",
            ". growth\n",
            ". firm\n",
            "  . Most correlated bigrams:\n",
            ". year old\n",
            ". self driving\n",
            "\n",
            "# 'entertainment' category:\n",
            "  . Most correlated unigrams:\n",
            ". tv\n",
            ". music\n",
            ". star\n",
            ". award\n",
            ". film\n",
            "  . Most correlated bigrams:\n",
            ". mr blair\n",
            ". self driving\n",
            "\n",
            "# 'politics' category:\n",
            "  . Most correlated unigrams:\n",
            ". tory\n",
            ". blair\n",
            ". election\n",
            ". party\n",
            ". labour\n",
            "  . Most correlated bigrams:\n",
            ". prime minister\n",
            ". mr blair\n",
            "\n",
            "# 'sport' category:\n",
            "  . Most correlated unigrams:\n",
            ". champion\n",
            ". coach\n",
            ". cup\n",
            ". match\n",
            ". game\n",
            "  . Most correlated bigrams:\n",
            ". self driving\n",
            ". year old\n",
            "\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the unigrams correspond well to their category. However, bigrams do not. If we get the bigrams in our features:"
      ],
      "metadata": {
        "id": "WD34oL5N2Bvt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bigrams"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tell bbc', 'last year', 'prime minister', 'mr blair', 'year old', 'say mr']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFmdhZXx2Bvt",
        "outputId": "3f44d12b-1f4a-479d-8d66-8e33faf944da"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see there are only six. This means the unigrams have more correlation with the category than the bigrams, and since we're restricting the number of features to the most representative 300, only a few bigrams are being considered."
      ],
      "metadata": {
        "id": "iOkhF9VM2Bvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save the files we'll need in the next steps:"
      ],
      "metadata": {
        "id": "Vu-UweBb2Bvt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "PathColab='/content/drive/MyDrive/AI Projects/Feature Engineering/ICE 1/Assignemnt_data/Pickles/'\n",
        "\n",
        "\n",
        "# X_train\n",
        "with open(PathColab+'X_test.pickle', 'wb') as output:\n",
        "    pickle.dump(X_train, output)\n",
        "    \n",
        "# X_test    \n",
        "with open(PathColab+'X_train.pickle', 'wb') as output:\n",
        "    pickle.dump(X_test, output)\n",
        "    \n",
        "# y_train\n",
        "with open(PathColab+'y_train.pickle', 'wb') as output:\n",
        "    pickle.dump(y_train, output)\n",
        "    \n",
        "# y_test\n",
        "with open(PathColab+'y_test.pickle', 'wb') as output:\n",
        "    pickle.dump(y_test, output)\n",
        "    \n",
        "# df\n",
        "with open(PathColab+'df.pickle', 'wb') as output:\n",
        "    pickle.dump(df, output)\n",
        "    \n",
        "# features_train\n",
        "with open(PathColab+'features_train.pickle', 'wb') as output:\n",
        "    pickle.dump(features_train, output)\n",
        "\n",
        "# labels_train\n",
        "with open(PathColab+'labels_train.pickle', 'wb') as output:\n",
        "    pickle.dump(labels_train, output)\n",
        "\n",
        "# features_test\n",
        "with open(PathColab+'features_test.pickle', 'wb') as output:\n",
        "    pickle.dump(features_test, output)\n",
        "\n",
        "# labels_test\n",
        "with open(PathColab+'labels_test.pickle', 'wb') as output:\n",
        "    pickle.dump(labels_test, output)\n",
        "    \n",
        "# TF-IDF object\n",
        "with open(PathColab+'tfidf.pickle', 'wb') as output:\n",
        "    pickle.dump(tfidf, output)"
      ],
      "outputs": [],
      "metadata": {
        "id": "W2RIoO5M2Bvt"
      }
    }
  ]
}